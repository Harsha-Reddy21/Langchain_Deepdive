# Auto Email Responder with Gmail MCP

## Overview
This project is an intelligent email response system that retrieves company policies and generates appropriate responses using Gmail MCP (Model Context Protocol). It leverages semantic search, a company knowledge base, and an LLM (OpenAI GPT) to auto-respond to emails, with support for batch processing and caching.

---

## Features
- **Gmail Integration:** Receive and send emails using the Gmail API.
- **Company Knowledge Base:** Policies, FAQs, and response templates are stored in `knowledge_base.json` and indexed with ChromaDB for semantic search.
- **Semantic Search:** Finds the most relevant policies, FAQs, or templates for a given query.
- **LLM Integration:** Uses OpenAI GPT to generate human-like, context-aware responses.
- **Auto-Responder:** Automatically generates and sends intelligent replies to emails.
- **Batch Processing:** Respond to multiple emails in a single batch request.
- **Prompt/Response Caching:** Uses Redis to cache semantic search results and LLM responses for efficiency.

---

## Setup

1. **Install dependencies:**
   ```sh
   pip install -r requirements.txt
   ```

2. **Set up credentials:**
   - Place your Gmail OAuth credentials in `credential.json` (see Google Cloud Console instructions).
   - Set your OpenAI API key in the environment:
     ```sh
     export OPENAI_API_KEY=your-openai-key
     ```

3. **Index the knowledge base:**
   ```sh
   python rag.py index
   ```

4. **Start Redis:**
   - Make sure Redis is running on `localhost:6379` (default).

5. **Run the MCP server:**
   ```sh
   python mcp_server.py
   ```

---

## Tools & Usage

### 1. Receive Emails
```python
get_mails(limit=10, query="from:example@gmail.com")
```

### 2. Send Email (Manual or Auto-Respond)
```python
# Manual
send_mail(to="x@y.com", subject="Test", body="Hello!")

# Auto-respond (semantic + LLM)
send_mail(to="x@y.com", subject="Policy Info", user_query="How many sick leaves do I get?", auto_respond=True)
```

### 3. Batch Auto-Respond
```python
batch_respond_to_emails([
    {"to": "a@company.com", "subject": "Leave", "user_query": "How many annual leaves do I have left?"},
    {"to": "b@company.com", "subject": "Remote Work", "user_query": "Can I work remotely from another city?"}
])
```

---

## Caching
- Semantic search and LLM responses are cached in Redis for fast repeated queries.
- You can clear the cache by flushing Redis if needed.

---

## Customization
- Update `knowledge_base.json` to add or modify company policies, FAQs, or templates.
- Tune the LLM prompt in `mcp_server.py` for your companyâ€™s tone or requirements.

---

